---

# LLM Profiler

ğŸš€ Herramienta para caracterizar modelos de lenguaje (LLMs) con monitorizaciÃ³n en tiempo real y anÃ¡lisis de recursos mediante **Ollama**, **Prometheus** y **Streamlit**.

---

## ğŸ“‹ Tabla de Contenidos

* [ğŸ¯ Resumen](#-resumen)
* [âœ¨ Funcionalidades](#-funcionalidades)
* [ğŸ”§ Requisitos Previos](#-requisitos-previos)
* [ğŸš€ InstalaciÃ³n](#-instalaciÃ³n)
* [ğŸ® Inicio RÃ¡pido](#-inicio-rÃ¡pido)
* [ğŸ“– Uso](#-uso)
* [ğŸ“Š Panel Interactivo](#-panel-interactivo)
* [ğŸ“ Estructura del Proyecto](#-estructura-del-proyecto)
* [ğŸ¤ Contribuir](#-contribuir)
* [ğŸ“„ Licencia](#-licencia)
* [ğŸ™ Agradecimientos](#-agradecimientos)

---

## ğŸ¯ Resumen

Este proyecto proporciona un marco robusto para:

* Evaluar el rendimiento de LLMs en mÃ¡quinas remotas
* Monitorizar recursos del sistema (CPU, GPU, memoria) durante inferencias
* Analizar respuestas del modelo y obtener puntuaciones
* Visualizar resultados mediante un panel interactivo

---

## âœ¨ Funcionalidades

* ğŸ”— **EvaluaciÃ³n Remota**: EjecuciÃ³n de pruebas en mÃ¡quinas remotas vÃ­a SSH
* ğŸ“¡ **MonitorizaciÃ³n en Tiempo Real**: Seguimiento de recursos con Prometheus
* ğŸ® **MÃ©tricas de GPU**: Uso y memoria de GPU monitorizados
* ğŸ“Š **Panel Interactivo**: VisualizaciÃ³n en Streamlit
* ğŸ¤– **Soporte Multimodelo**: EvaluaciÃ³n de varios modelos mediante Ollama
* ğŸ“ **Registro Detallado**: Logs extensos para depuraciÃ³n y anÃ¡lisis

---

## ğŸ”§ Requisitos Previos

* Python 3.8+
* Acceso SSH a la(s) mÃ¡quina(s) remota(s)
* [Prometheus](https://prometheus.io) (para recolecciÃ³n de mÃ©tricas)
* [Ollama](https://ollama.com) (para inferencia LLM)

---

## ğŸš€ InstalaciÃ³n

### 1. Instalar Prometheus

#### macOS (ARM64):

```bash
curl -LO https://github.com/prometheus/prometheus/releases/download/v3.3.1/prometheus-3.3.1.darwin-arm64.tar.gz
tar -xvf prometheus-3.3.1.darwin-arm64.tar.gz
sudo mv ./prometheus-3.3.1.darwin-arm64/prometheus /usr/local/bin/
sudo mv ./prometheus-3.3.1.darwin-arm64/promtool /usr/local/bin/
```

#### Linux (AMD64):

```bash
curl -LO https://github.com/prometheus/prometheus/releases/download/v3.3.1/prometheus-3.3.1.linux-amd64.tar.gz
tar -xvf prometheus-3.3.1.linux-amd64.tar.gz
sudo mv ./prometheus-3.3.1.linux-amd64/prometheus /usr/local/bin/
sudo mv ./prometheus-3.3.1.linux-amd64/promtool /usr/local/bin/
```

#### Verificar instalaciÃ³n:

```bash
prometheus --version
```

#### InstalaciÃ³n alternativa (local):

```bash
export PATH=$HOME/prometheus/:$PATH
```

---

### 2. Configurar el entorno Python

```bash
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
pip install -r requirements.txt
```

---

## ğŸ® Inicio RÃ¡pido

1. Configura tu experimento:

```bash
python3 llm_llama_eval.py --help
```

2. Ejecuta la evaluaciÃ³n:

```bash
python3 llm_llama_eval.py --host <ip-remota> --user <usuario> --key <ruta-clave.pem>
```

3. Lanza el panel:

```bash
streamlit run data_representation/dashboard.py
```

---

## ğŸ“– Uso

### Script principal: `llm_llama_eval.py`

```bash
python3 llm_llama_eval.py [OPCIONES]
```

| OpciÃ³n                         | Obligatorio | DescripciÃ³n                       |
| ------------------------------ | ----------- | --------------------------------- |
| `-i`, `--ip-address TEXT`      | âœ…           | IP de la mÃ¡quina remota           |
| `-u`, `--user TEXT`            | Opcional    | Usuario SSH                       |
| `-p`, `--password TEXT`        | Opcional    | ContraseÃ±a SSH                    |
| `-pk`, `--private-key TEXT`    | Opcional    | Ruta a la clave privada (.pem)    |
| `-ov`, `--ollama-version TEXT` | Opcional    | VersiÃ³n de Ollama                 |
| `-nv`, `--node-version TEXT`   | Opcional    | VersiÃ³n del Node Exporter         |
| `-ro`, `--reinstall-ollama`    | Opcional    | Fuerza la reinstalaciÃ³n de Ollama |

#### Ejemplos:

**Con clave privada**:

```bash
python3 llm_llama_eval.py -i 192.168.1.100 -u ubuntu -pk ./clave.pem
```

**Con contraseÃ±a**:

```bash
python3 llm_llama_eval.py -i 192.168.1.100 -u ubuntu -p mipassword
```

**Con versiones especÃ­ficas**:

```bash
python3 llm_llama_eval.py -i 192.168.1.100 -u ubuntu -pk ./clave.pem -ov v0.1.32 -nv v1.5.0
```

---

### Script del Panel: `dashboard.py`

```bash
streamlit run data_representation/dashboard.py [OPCIONES]
```

| OpciÃ³n              | DescripciÃ³n                                      |
| ------------------- | ------------------------------------------------ |
| `-d`, `--directory` | Directorio que contiene los archivos de mÃ©tricas |
| `-h`, `--help`      | Muestra la ayuda                                 |

---

## ğŸ“Š Panel Interactivo

El panel proporciona:

* â±ï¸ MÃ©tricas de rendimiento: tiempos de respuesta, puntuaciones
* ğŸ“ˆ Uso de recursos: CPU, memoria, GPU
* ğŸ§  Comparativas: anÃ¡lisis lado a lado de modelos
* ğŸ“¤ ExportaciÃ³n: descarga de resultados en varios formatos

### Modos de ejecuciÃ³n:

#### OpciÃ³n 1: Subida manual

```bash
streamlit run data_representation/dashboard.py
```

Abre en [http://localhost:8501](http://localhost:8501) y sube manualmente tus archivos en este orden:

1. MÃ©tricas de Ollama
2. Puntuaciones de modelos
3. MÃ©tricas de Prometheus
4. InformaciÃ³n general

---

#### OpciÃ³n 2: Carga automÃ¡tica desde directorio

```bash
streamlit run data_representation/dashboard.py -- --directory ./experiment_results/2025-07-11-10-30-15
```

Archivos requeridos en el directorio:

* `ollama_metrics.csv` o `ollama_metrics*.csv`
* `models_score.csv` o `*score*.csv`
* `prometheus_metrics.csv` o `prometheus_metrics*.csv`
* `general_info.txt` o `general_info*.txt`

---

## ğŸ“ Estructura del Proyecto

```
llm-eval-TFG/
â”œâ”€â”€ data_representation/     # Panel e interfaz
â”‚   â””â”€â”€ dashboard.py
â”œâ”€â”€ experiment_results/      # Resultados almacenados
â”œâ”€â”€ gpu_exporter/            # RecolecciÃ³n de mÃ©tricas GPU
â”œâ”€â”€ logger/                  # Utilidades de logging
â”œâ”€â”€ metrics/                 # MÃ©tricas recolectadas
â”œâ”€â”€ ollama/                  # IntegraciÃ³n con Ollama
â”œâ”€â”€ prometheus/              # ConfiguraciÃ³n de Prometheus
â”œâ”€â”€ versions/                # GestiÃ³n de versiones
â”œâ”€â”€ llm_llama_eval.py        # Script principal
â”œâ”€â”€ requirements.txt         # Dependencias Python
â””â”€â”€ README.md                # Este archivo
```

---

## ğŸ¤ Contribuir

1. Haz un fork del repositorio
2. Crea una rama de feature:

   ```bash
   git checkout -b feature/nueva-funcionalidad
   ```
3. Realiza tus cambios y haz commit:

   ```bash
   git commit -m "Agrega nueva funcionalidad"
   ```
4. Haz push:

   ```bash
   git push origin feature/nueva-funcionalidad
   ```
5. Abre un Pull Request

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ licenciado bajo la [Licencia MIT](LICENSE).

---

## ğŸ™ Agradecimientos

* [Prometheus](https://prometheus.io) por la monitorizaciÃ³n
* [Ollama](https://ollama.com) por la inferencia de LLMs
* [Streamlit](https://streamlit.io) por la interfaz visual

---
